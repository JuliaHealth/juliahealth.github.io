[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the JuliaHealth Blog",
    "section": "",
    "text": "Yes! We use GoatCounter which is an open-source web analytics platform. It has a very strong privacy policy that forbids tracking users."
  },
  {
    "objectID": "about.html#can-i-trust-my-privacy",
    "href": "about.html#can-i-trust-my-privacy",
    "title": "About the JuliaHealth Blog",
    "section": "",
    "text": "Yes! We use GoatCounter which is an open-source web analytics platform. It has a very strong privacy policy that forbids tracking users."
  },
  {
    "objectID": "posts/jay-gsoc/gsoc-2024-fellows.html",
    "href": "posts/jay-gsoc/gsoc-2024-fellows.html",
    "title": "GSoC ‚Äô24: Developing Tooling for Observational Health Research in Julia",
    "section": "",
    "text": "I am Jay Sanjay, and I am pursuing a Bachelor‚Äôs degree in Computational Sciences and Engineering at the Indian Institute of Technology (IIT) in Hyderabad, India. Coming from a mathematics and data analysis background, I was initially introduced to Julia at my university lectures. Later, I delved more into the language and the JuliaHealth community - an intersection of Julia, Health Research, Data Sciences, and Informatics. Here, I met some of the great folks in JuliaHealth and I decided to take it on as a full-fledged summer project. In this blog, I will briefly describe what my project is and what I did as a part of it.\n\nIf you want to know more about me, you can connect with me on LinkedIn and follow me on GitHub"
  },
  {
    "objectID": "posts/jay-gsoc/gsoc-2024-fellows.html#what-is-observational-health-research",
    "href": "posts/jay-gsoc/gsoc-2024-fellows.html#what-is-observational-health-research",
    "title": "GSoC ‚Äô24: Developing Tooling for Observational Health Research in Julia",
    "section": "What Is Observational Health Research?",
    "text": "What Is Observational Health Research?\nObservational Health Research refers to studies that analyze real-world data (such as patient medical claims, electronic health records, etc.) to understand patient health. These studies often encompass a vast amount of data concerning patient care. An outstanding challenge here is that these datasets can become very complex and grow large enough to require advanced computing methods to process this information."
  },
  {
    "objectID": "posts/jay-gsoc/gsoc-2024-fellows.html#what-are-patient-pathways",
    "href": "posts/jay-gsoc/gsoc-2024-fellows.html#what-are-patient-pathways",
    "title": "GSoC ‚Äô24: Developing Tooling for Observational Health Research in Julia",
    "section": "What Are Patient Pathways?",
    "text": "What Are Patient Pathways?\nPatient pathways refer to the journey that patients with specific medical conditions undergo in terms of their treatment. This concept goes beyond simple drug uptake statistics and looks at the sequence of treatments patients receive over time, including first-line treatments and subsequent therapies. Understanding patient pathways is essential for analyzing treatment patterns, adherence to clinical guidelines, and the disbursement of drugs. To analyze patient pathways, one would typically use real-world data from sources such as electronic health records, claims data, and registries. However, barriers such as data interoperability and resource requirements have hindered the full utilization of real-world data for this purpose.\nSo to address these challenges we (the JuliaHealth organization and I) want to develop a set of tools to extract and analyze these patient pathways. These sets of tools are based on the Observational Medical Outcomes Partnership (OMOP) Common Data Model, which standardizes healthcare data to promote interoperability."
  },
  {
    "objectID": "posts/jay-gsoc/gsoc-2024-fellows.html#setting-up-the-package-in-juliahealth-channel",
    "href": "posts/jay-gsoc/gsoc-2024-fellows.html#setting-up-the-package-in-juliahealth-channel",
    "title": "GSoC ‚Äô24: Developing Tooling for Observational Health Research in Julia",
    "section": "1. Setting Up the Package in JuliaHealth Channel",
    "text": "1. Setting Up the Package in JuliaHealth Channel\nInitially, there was no package as such for generating pathways, so I had to build it from scratch. First, I created the repository with the name OMOPCDMPathways.jl. Once the repository was created, we needed to have a skeleton for a standard Julia repository. For this, we used the PkgTemplates.jl this provided a basic skeleton for the repository that included - folders for test suites, documentation, src code files, GitHub files, README and LICENSE file, TOML and citation files. All this we can further edit and modify as per our work. By default, PkgTemplate.jl uses Documenter.jl for the documentation part but as suggested and discussed with my mentor we decided to shift to DocumenterVitepress.jl for the documentation part. However, we still faced some deployment issues in the new documentation due to a few mistakes in the make.jl file, thanks to Anshul Singhvi for helping fix the Deployment issues with DocumenterVitepress. With this, we were ready with the documentation set up and fully functional. After we had shifted to DocumenterVitepress the main task now was to host the documentation, this was done using Github-Actions, detailed steps for hosting are provided at this page. Then we added the CodeCov to our package by triggering it via a dummy function and a corresponding test case for it. Also, the CI for the package was set up with it. And, now finally the repository was ready with test coverage, CI, and documentation fully functional repository ready. Here‚Äôs some snapshots of the documentation set-up:\n\n\nInitial documentation with Documenter.jl\n\n\n\nNew documentation using DocumenterVitepress.jl\n\nSo, as a part of it, I created this documentation which provides detailed steps for converting docs from Documenter to DocumenterVitepress."
  },
  {
    "objectID": "posts/jay-gsoc/gsoc-2024-fellows.html#loading-the-postgresql-database",
    "href": "posts/jay-gsoc/gsoc-2024-fellows.html#loading-the-postgresql-database",
    "title": "GSoC ‚Äô24: Developing Tooling for Observational Health Research in Julia",
    "section": "2. Loading the PostgreSQL Database",
    "text": "2. Loading the PostgreSQL Database\nThe main database we worked on/built analysis was the freely available OMOPCDM Database. The Database was formatted within a PostgreSQL database with installation instructions here are some instructions on how to set up Postgres in a Linux machine. However, I was provided with some more extra synthetic data from my mentor for further testing of the functionalities. Being a very large database we had to strategically download it further, my mentor helped me in setting up the Postgres on my local machine. Once, the database was set up proper testing was performed on it to check if things were as expected. With this, we were done with the database setup as well and could finally dive into the actual code logic for the Pathways synthesis."
  },
  {
    "objectID": "posts/jay-gsoc/gsoc-2024-fellows.html#testing-and-development-setup-on-my-local-computer",
    "href": "posts/jay-gsoc/gsoc-2024-fellows.html#testing-and-development-setup-on-my-local-computer",
    "title": "GSoC ‚Äô24: Developing Tooling for Observational Health Research in Julia",
    "section": "3. Testing and Development setup on my local computer",
    "text": "3. Testing and Development setup on my local computer\nTo get a proper environment for functionality creation and concurrent testing we required a proper testing setup so that we could test the new functions made at the same time. This was done using Revise.jl, which helps to keep Julia sessions running without frequent restarts when making changes to code. It allowed me to edit my code, update packages, or switch git branches during a session, with changes applied immediately in the next command. My mentor helped me set it up, added Revise.jl to the global Julia environment, also PackageCompatUI that provides a terminal text interface to the [compat] section of a Julia Project.toml file, and finally made a Julia script by the name ‚Äústartup.jl‚Äù out of it. This script was then added to /home/jay-sanjay/.julia/config/ path in my local computer.\nHere is the sample for the startup.jl file:\nusing PackageCompatUI\nusing PkgTemplates\nusing Revise\n\n###################################\n# HELPER FUNCTIONS\n###################################\nfunction template()\n    Template(;\n        user=\"jay-sanjay\",\n        dir=\"~/FOSS\",\n        authors=\"jaysanjay &lt;jaysanjay@gmail.com&gt; and contributors\",\n        julia=v\"1.6\",\n        plugins=[\n            ProjectFile(; version=v\"0.0.1\"),\n            Git(),\n            Readme(),\n            License(; name=\"MIT\"),\n            GitHubActions(; extra_versions=[\"1.6\", \"1\", \"nightly\"]),\n            TagBot(),\n            Codecov(),\n            Documenter{GitHubActions}(),\n            Citation(; readme = true),\n            RegisterAction(),\n            BlueStyleBadge(),\n            Formatter(;style = \"blue\")\n        ],\n    )\nend"
  },
  {
    "objectID": "posts/jay-gsoc/gsoc-2024-fellows.html#selecting-treatments-of-interest",
    "href": "posts/jay-gsoc/gsoc-2024-fellows.html#selecting-treatments-of-interest",
    "title": "GSoC ‚Äô24: Developing Tooling for Observational Health Research in Julia",
    "section": "4. Selecting Treatments of Interest",
    "text": "4. Selecting Treatments of Interest\nSo, as a part of this, we used the previously mentioned research paper and discussion with the mentors we came up with logic for it. The first thing to do was to determine the moment in time from which selected treatments of interest should be included in the treatment pathway. The default is all treatments starting after the index date of the target cohort. For example, for a target cohort consisting of newly diagnosed patients, treatments after the moment of first diagnosis are included. However, it would also be desirable to include (some) treatments before the index date, for instance in case a specific disease diagnosis is only confirmed after initiating treatment. Therefore, periodPriorToIndex specifies the period (i.e.¬†number of days) before the index date from which treatments should be included. We have created two dispatches for this function. After that proper testing and documentation are also added.\nA basic implementation for it is:\n\nConstruct a SQL query to select cohort_definition_id, subject_id, and cohort_start_date from a specified table, filtering by cohort_id.\nThe SQL query construction and execution was done using the FunSQL.jl library, in the below-shown manner:\n\nsql = From(tab) |&gt;\n            Where(Fun.in(Get.cohort_definition_id, cohort_id...)) |&gt;\n            Select(Get.cohort_definition_id, Get.subject_id, Get.cohort_start_date) |&gt;\n            q -&gt; render(q, dialect=dialect)\n\nExecutes the constructed SQL query using a database connection, fetching the results into a data frame.\nIf the DataFrame is not empty, convert cohort_start_date to DateTime and subtract date_prior from each date, then return the modified DataFrame.\n\nThis was then be called this:\nperiod_prior_to_index(\n        cohort_id = [1, 1, 1, 1, 1], \n        conn; \n        date_prior = Day(100), \n        tab=cohort\n    )"
  },
  {
    "objectID": "posts/jay-gsoc/gsoc-2024-fellows.html#filters-applied",
    "href": "posts/jay-gsoc/gsoc-2024-fellows.html#filters-applied",
    "title": "GSoC ‚Äô24: Developing Tooling for Observational Health Research in Julia",
    "section": "5. Filters Applied",
    "text": "5. Filters Applied\nAfter this, we where needed to get the patient‚Äôs database filtered more finely so that there are minimal variations that can be ignored. The duration of the above extracted event eras may vary a lot and it can be preferable to limit to only treatments exceeding a minimum duration. Hence, minEraDuration speciÔ¨Åes the minimum time an event era should last to be included in the analysis. All these implementations were more of Dataframe manipulation where I used DataFrames.jl package.\nAfter that proper testing and documentation are also added.\nA basic implementation for the minEraDuration is: It filters the treatment history DataFrame to retain only those rows where the duration between drug_exposure_end and drug_exposure_start is at least minEraDuration. This function can be used as follows:\n#| eval: false \n\ncalculate_era_duration(test_df, 920000)\n\n#= ... =#\n\n4√ó3 DataFrame\n Row ‚îÇ person_id  drug_exposure_start  drug_exposure_end \n     ‚îÇ Int64      Float64              Int64             \n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n   1 ‚îÇ         1           -3.7273e8          -364953600\n   2 ‚îÇ         1            2.90304e7           31449600\n   3 ‚îÇ         1           -8.18208e7          -80006400\n   4 ‚îÇ         1            1.32918e9         1330387200\nAnother filter we worked on is the EraCollapse. So, let‚Äôs suppose a case where an individual receives the same treatment for a long period of time (e.g.¬†need for chronic treatment). Then it‚Äôs highly likely that the person would require refills. Now as patients are not 100% adherent, there might be a gap between two subsequent event eras. Usually, these eras are still considered as one treatment episode, and the eraCollapseSize deals with the maximum gap within which two eras of the same event cohort would be collapsed into one era (i.e.¬†seen as a continuous treatment instead of a stop and re-initiation of the same treatment). After that proper testing and documentation are also added.\nA basic implementation for the eraCollapseSize is: (a) Sorts the data frame by event_start_date and event_end_date. (b) Calculates the gap between each era and the previous era. (c) Filters out rows with gap_same &gt; eraCollapseSize.\nThese functions can be used as follows:\n#| eval: false \n\n#= ... =#\n\nEraCollapse(treatment_history = test_df, eraCollapseSize = 400000000)\n4√ó4 DataFrame\n Row ‚îÇ person_id  drug_exposure_start  drug_exposure_end  gap_same   \n     ‚îÇ Int64      Float64              Int64              Float64    \n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n   1 ‚îÇ         1           -5.33347e8         -532483200  -1.86373e9\n   2 ‚îÇ         1           -3.7273e8          -364953600   1.59754e8\n   3 ‚îÇ         1           -8.18208e7          -80006400   2.83133e8\n   4 ‚îÇ         1            2.90304e7           31449600   1.09037e8"
  },
  {
    "objectID": "posts/jay-gsoc/gsoc-2024-fellows.html#treatment-history-of-the-patients",
    "href": "posts/jay-gsoc/gsoc-2024-fellows.html#treatment-history-of-the-patients",
    "title": "GSoC ‚Äô24: Developing Tooling for Observational Health Research in Julia",
    "section": "6. Treatment History of the Patients",
    "text": "6. Treatment History of the Patients\nThe create_treatment_history function constructs a detailed treatment history for patients in a target cohort by processing and filtering event cohort data from a given DataFrame. It begins by isolating the target cohort based on its cohort_id, adding a new column for the index_year derived from the cohort‚Äôs start date. Then, it selects relevant event cohorts based on a provided list of cohort IDs and merges them with the target cohort on the subject_id to associate events with individuals in the target group. The function applies different filtering criteria depending on whether the user is interested in treatments starting or ending within a specified period before the target cohort‚Äôs start date (defined by periodPriorToIndex). It keeps only the event cohorts that match the filtering condition, ensuring that only relevant treatments are considered. After filtering, the function calculates time gaps between consecutive cohort events for each patient, adding these gaps to the DataFrame. The final DataFrame provides a history of treatments, including the dates of events and the time intervals between them, offering a clear timeline of treatment for each patient. After that proper testing and documentation are also added."
  },
  {
    "objectID": "posts/jay-gsoc/gsoc-2024-fellows.html#combinationwindow-functionality-to-combine-overlapping-treatments",
    "href": "posts/jay-gsoc/gsoc-2024-fellows.html#combinationwindow-functionality-to-combine-overlapping-treatments",
    "title": "GSoC ‚Äô24: Developing Tooling for Observational Health Research in Julia",
    "section": "7. CombinationWindow Functionality To Combine Overlapping Treatments",
    "text": "7. CombinationWindow Functionality To Combine Overlapping Treatments\nNow once we have the filtering of the treatments done, we need to combine the overlapping treatments based on some set of rules. The combinationWindow specifies the time that two event eras need to overlap to be considered a combination treatment. If there are more than two overlapping event eras, we sequentially combine treatments, starting from the Ô¨Årst two overlapping event eras.\nThe combination_Window function processes a patient‚Äôs treatment history by identifying overlapping treatment events and combining them into continuous treatment periods based on certain rules. It first converts event_cohort_id into strings and sorts the treatment data by person_id, event_start_date, and event_end_date. The helper function selectRowsCombinationWindow calculates gaps between consecutive treatments, marking rows where treatments overlap or occur too closely. In the main loop, the function checks these overlaps and gaps against a specified combinationWindow. If treatments overlap (or nearly overlap), the function adjusts the treatment periods by either merging adjacent rows or splitting rows to create continuous treatment periods. The process continues until all overlapping treatments are combined into one, creating an updated and accurate treatment history. The function ensures the final output reflects realistic treatment windows by handling special cases where gaps between treatments are smaller than the treatment durations themselves.\nIt mainly covers the three cases mentioned in the R-research paper:\n\nSwitch Case:\nCondition: If the gap between the two treatment events is smaller than the combinationWindow, but the gap is not equal to the duration of either event. Action: The event_end_date of the previous treatment is set to the event_start_date of the current treatment. This effectively ‚Äúshifts‚Äù the previous treatment‚Äôs end date to eliminate the gap, merging the treatments into one continuous period. Purpose: This ensures that treatment gaps that are too small (less than combinationWindow) are treated as part of the same treatment window.\n#| eval: false \n\n#= ... =#\n\nif -gap_previous &lt; combinationWindow && !(-gap_previous in [duration_era, prev_duration_era])\n    treatment_history[i-1, :event_end_date] = treatment_history[i, :event_start_date]\nHere is the pictorial representation for the same: \n\n\nFRFS (First Row, First Shortened):\nCondition: If the gap is larger than or equal to the combinationWindow, or the gap equals the duration of one of the two treatments, and the first treatment ends before or on the same date as the second treatment. Action: A new row is created where the second treatment‚Äôs event_end_date is set to the end date of the first treatment. This preserves the overlap but ensures that the earlier treatment period stays intact. Purpose: This prevents unnecessary truncation of the first treatment if it spans the entire overlap window.\n#| eval: false \n\n#= ... =#\n\nelseif -gap_previous &gt;= combinationWindow || -gap_previous in [duration_era, prev_duration_era]\n    if treatment_history[i-1, :event_end_date] &lt;= treatment_history[i, :event_end_date]\n        new_row = deepcopy(treatment_history[i, :])\n        new_row.event_end_date = treatment_history[i-1, :event_end_date]\n        append!(treatment_history, DataFrame(new_row'))\nHere is the pictorial representation for the same: \n\n\nLRFS (Last Row, First Shortened):\nCondition: If the gap is larger than or equal to the combinationWindow, or the gap equals the duration of one of the treatments, and the first treatment ends after the second treatment. Action: The current treatment‚Äôs event_end_date is adjusted to match the event_end_date of the previous treatment. Purpose: This handles cases where the second treatment‚Äôs window should be shortened to prevent overlap with the previous treatment, merging them into a single continuous window.\n#| eval: false \n\n#= ... =#\n\nelse\n    treatment_history[i, :event_end_date] = treatment_history[i-1, :event_end_date]\nHere is the pictorial representation for the same: \n\nNote: However, There are a few things left to cover here, most of which are the documentation and writing the test suite for the same."
  },
  {
    "objectID": "posts/jay-gsoc/gsoc-2024-fellows.html#organizing-meetings-and-communication",
    "href": "posts/jay-gsoc/gsoc-2024-fellows.html#organizing-meetings-and-communication",
    "title": "GSoC ‚Äô24: Developing Tooling for Observational Health Research in Julia",
    "section": "1. Organizing Meetings and Communication",
    "text": "1. Organizing Meetings and Communication\nThroughout the project, I regularly met with my mentor, [Jacob Zelko], and co-mentor, [Mounika], via weekly Zoom calls to discuss progress and seek guidance. During these meetings, we reviewed my work, identified areas where I needed help, and set clear goals for the upcoming weeks. We used Trello to organize and track these goals, ensuring that nothing was overlooked. My mentors provided detailed insights into specific technical aspects and guided me through the logic behind various functions. Outside of our scheduled meetings, they were always available for quick queries via Slack, ensuring constant support."
  },
  {
    "objectID": "posts/jay-gsoc/gsoc-2024-fellows.html#personal-documentation",
    "href": "posts/jay-gsoc/gsoc-2024-fellows.html#personal-documentation",
    "title": "GSoC ‚Äô24: Developing Tooling for Observational Health Research in Julia",
    "section": "2. Personal Documentation",
    "text": "2. Personal Documentation\nIn addition to the notes from our meetings, I maintained personal documentation where I recorded every step I took, including the challenges I faced and the mistakes I made. This helped me reflect on my progress and stay organized throughout the fellowship. Following my selection for GSoC 2024, I also published a blog post on Medium to share my journey and experiences with the Julia Language community."
  },
  {
    "objectID": "posts/jay-gsoc/gsoc-2024-fellows.html#contributions-to-the-rest-of-the-juliahealth-repositories",
    "href": "posts/jay-gsoc/gsoc-2024-fellows.html#contributions-to-the-rest-of-the-juliahealth-repositories",
    "title": "GSoC ‚Äô24: Developing Tooling for Observational Health Research in Julia",
    "section": "3. Contributions To the Rest of the JuliaHealth Repositories",
    "text": "3. Contributions To the Rest of the JuliaHealth Repositories\nEarlier I have contributed a lot to the OMOPCDMCohortCreator.jl including adding new functionalities writing test suites, adding blogs including - Patient Pathways within JuliaHealth. Apart from that I also initiated 3 new releases of this package."
  },
  {
    "objectID": "posts/michela-gsoc/Michela_JSoC.html",
    "href": "posts/michela-gsoc/Michela_JSoC.html",
    "title": "GSoC ‚Äô24: IPUMS.jl Small Project",
    "section": "",
    "text": "Hello! üëã\nHi! I am Michela, I have a Master‚Äôs degree in Physics of Complex Systems and I am currently working as a software engineer in Rome, where I am from. During my studies, I became interested in the use of modeling and AI methods to improve healthcare and how these tools can be used to better understand how cultural and social backgrounds influence the health of individuals. I am also interested in the computational modeling of the brain and the human body and its implications for a better understanding of certain pathological conditions.\nWith these motivations in mind, I heard about Google Summer of Code. Since I had studied Julia in some courses and given that the language is expanding rapidly, I decided to find a project within Julia. As a result, I found the project of Jacob Zelko (@TheCedarPrince) to start this experience.\n\nIf you want to learn more about me, you can connect with me here: LinkedIn, GitHub\n\n\n\nProject Description\nIPUMS is the ‚Äúworld‚Äôs largest available single database of census microdata‚Äù, providing survey and census data from around the world. It includes several projects that provide a wide variety of datasets. The information and data collected by IPUMS are useful for comparative research, as well as for the analysis of individuals in their life contexts. These data can be used to create a more comprehensive dataset that will facilitate research on the social determinants of health for different types of diseases, social communities, and geographical areas.\n\n\nTo learn more about IPUMS, visit the website\n\n\n\nTasks and Goals\nThe primary objectives of this proposal are to:\n\nDevelop a native Julia package to interact with the APIs available around the datasets IPUMS provides.\nProvide useful utilities within this package for manipulating IPUMS datasets.\nCompose this package with the wider Julia ecosystem to enable novel research in health, economics, and more.\n\nTo achieve this, the work was distributed as follows:\n\nExpand some of the functionality developed in ipumsr IPUMS NHGIS\n\nCreate a link between OpenAPI documentation and the functions internally used in IPUMS.jl: updating already present functions, determining if updating is needed, and testing them\nDevelop functionality similar to the get_metadata_nghis function present in ipumsr\n\nUpdate IPUMS documentation\n\nSet up and deploy DocumenterVitepress.jl\n\nWrite a blog post on how IPUMS.jl can be composed within the ecosystem.\n\n\n\n\nHow the work was done\nThe first task was to migrate documents from Documenter to DocumenterVitepress.This issue aims to support the significant refactoring underway across JuliaHealth, aimed at improving the discoverability and cohesion of the JuliaHealth ecosystem, particularly about documentation. This issue is intended to create a more attractive entry point for new Julia users interested in health research within the Julia community. To accomplish this task, a dependency of DocumenterVitepress was added to the docs directory of the IPUMS.jl repository. Once this was done, the Documenter.jl make.jl file was migrated into a DocumenterVitepress.jl make.jl file. Working on the make.jl file, the pages structure were added to the web page explaining the IPUMS.jl package. With this in mind, those were added: 1. Home: to explain the main purpose of the package 2. Workflows: to explain the working process 3. How to: to give general information 4. Tutorials: to show how to use IPUMS.jl\n5. Examples: some examples of activities 6. Mission: to explain why the package is useful for the community 7. References: references used to write the pages.\nThis first task takes some time, especially setting up GitHub and cloning the repository locally. At this point, my experience with GitHub was really limited and I had to learn how to use the Git environment from scratch, for example how to do continuous integration (to commit code to a shared repository), documentation release and merge, and local testing. I found the support of my mentors and searching for material online was really helpful.\nThe second task was to update the documentation of IPUMS.jl by modifying the functionality within the model folder in the IPUMS.jl folder. The main aim of this task was to a description of the function and its attributes, an example of possible implementation and result, and finally to show how to use it. The documentation to be updated as of several types of functions: 1. Data extract 2. Data set 3. Data Table 4. Time series table 5. Error 6. Shapefile. Each of these macro-categories (from 1 to 4) contains a set of functions, each signaling the different expected output and specific purpose. Information about what each function does, and the meaning of each specific input variable, has been found on the IPUMS website and references have been made in the written documentation.\n\n\nHow to work with IPUMS\nAfter writing down the description of the function and the inputs, examples were formulated, starting from the IPUMS website: when you register at IPUMS, an API key is given. which is used, among other things, to run pre-written code on the website. This code contains examples of these functions, and these examples have been adapted by changing some input values and adapting them to work in the Julia framework. The latter task was done by simply rewriting some structures, such as dictionaries, maps, or lists, in the Julia language. Here is a small guide on how to set up working with the API: 1. Create an IPUMS account 2. Log in to your account 3. Copy the API key, which can be obtained from the website 4. Use the key to run the code that is already available on the IPUMS Developer Portal, where you will also find information about the variables and packages.\n\n\nFunctions testing\nA final task was to test the functions in the ‚Äòapi_IPUMSAPI.jl‚Äô file. In this file, the function to be tested and other functions are defined and the most important ones are extracted to be available in the available throughout the framework. Some of the functions to be tested were the following:\n\nmetadata_nhgis_data_tables_get\nmetadata_nhgis_datasets_dataset_data_tables_data_table_get\nmetadata_nhgis_datasets_dataset_get\nmetadata_nhgis_datasets_get\n\nBefore working on the Julia files, testing and understanding the original R function was done using R studio.\n\nEach function was then tested using the API key from the IPUMS registration as well as other input examples taken from the documentation or the IPUMS website. or from the IPUMS website. All functions were displayed successfully, giving the expected result, so it can be concluded that the translation from R to Julia is successful.\n\n\nCode\nusing IPUMS\nusing OpenAPI\n\napi_key = \"insert your key here\"\n\nversion = \"2\"\npage_number = 1\npage_size = 2500\n#media_type = \n\napi = IPUMSAPI(\"https://api.ipums.org\", Dict(\"Authorization\" =&gt; api_key));\n\nres1 = metadata_nhgis_data_tables_get(api, version)\n\nres2 = metadata_nhgis_datasets_dataset_get(api, \"2022_ACS1\", \"2\");\n\nres3 = metadata_nhgis_datasets_dataset_data_tables_data_table_get(api, \"2022_ACS1\",\"B01001\", \"2\");\n\nres4 = metadata_nhgis_datasets_get(api, \"2\");\n\n\nAn example of the output is:\n. . .\n\n{\n  \"name\": \"NT1\",\n  \"nhgisCode\": \"AAA\",\n  \"description\": \"Total Population\",\n  \"universe\": \"Persons\",\n  \"sequence\": 1,\n  \"datasetName\": \"1790_cPop\",\n  \"nVariables\": [\n    1\n  ]\n}\n\n. . .\n\n\nAccomplished Goals and Future Development\nThe project was a 90-hour small project and during this time the documentation was completed and the testing of the metadata function was done, as well as the migration from Documenter.jl to DocumenterVitepress.jl. During these months some things took longer than I expected because of some problems that occurred, so some things were missing in relation to the original plan. However, this time was useful for learning new things: - I saw how to work with a package under development, how to work with large datasets, and how to write documentation - I had the opportunity to better understand how to work with Git and GitHub - I learned some new things about R, which was a completely unknown language to me. - I deepened my knowledge of Julia, a language I had worked with during my time at university. - I had the chance to work on a large open-source project, to be part of a large community, and to learn how to communicate with it efficiently.\nA special thanks goes to my mentors, Jacob Zelko and Krishna Bhogaonker, for helping me through this process.\nFuture developments of this work could include deepening the work that my mentors and I have started, with the possibility of integrating this package with other machine learning packages in Julia and, from there, doing new analyses of the data in terms of social and geographical implications for health.\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{rocchetti2024,\n  author = {Rocchetti, Michela},\n  title = {GSoC ‚Äô24: {IPUMS.jl} {Small} {Project}},\n  date = {2024-08-26},\n  url = {https://juliahealth.org/JuliaHealthBlog/posts/michela-gsoc/Michela_JSoC.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRocchetti, Michela. 2024. ‚ÄúGSoC ‚Äô24: IPUMS.jl Small\nProject.‚Äù August 26, 2024. https://juliahealth.org/JuliaHealthBlog/posts/michela-gsoc/Michela_JSoC.html."
  },
  {
    "objectID": "posts/dummy/index.html",
    "href": "posts/dummy/index.html",
    "title": "Dummy Post",
    "section": "",
    "text": "Seciton 1\nSmall dummy blog post\n\n\nCode\n2 + 2\n\n\n4\n\n\n\n\nCode\nprintln(2 + 2)\n\n\n4\n\n\n\n\nSection 2\n\n\nSection 3\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{2024,\n  author = {, Foobar},\n  title = {Dummy {Post}},\n  date = {2024-06-22},\n  url = {https://juliahealth.org/JuliaHealthBlog/posts/dummy/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nFoobar. 2024. ‚ÄúDummy Post.‚Äù June 22, 2024. https://juliahealth.org/JuliaHealthBlog/posts/dummy/."
  },
  {
    "objectID": "posts/ryan-gsoc/Ryan_GSOC.html",
    "href": "posts/ryan-gsoc/Ryan_GSOC.html",
    "title": "GSoC ‚Äô24: Enhancements to KomaMRI.jl GPU Support",
    "section": "",
    "text": "Hi! üëã\nI am Ryan, an MS student currently studying computer science at the University of Wisconsin-Madison. Looking for a project to work on this summer, my interest in high-performance computing and affinity for the Julia programming language drew me to Google Summer of Code, where I learned about this project opportunity to work on enhancing GPU support for KomaMRI.jl.\nIn this post, I‚Äôd like to summarize what I did this summer and everything I learned along the way!\n\nIf you want to learn more about me, you can connect with me here: LinkedIn, GitHub\n\n\n\nWhat is KomaMRI?\nKomaMRI is a Julia package for efficiently simulating Magnetic Resonance Imaging (MRI) acquisitions. MRI simulation is a useful tool for researchers, as it allows testing new pulse sequences to analyze the signal output and image reconstruction quality without needing to actually take an MRI, which may be time or cost-prohibitive.\nIn contrast to many other MRI simulators, KomaMRI.jl is open-source, cross-platform, and comes with an intuitive user interface (To learn more about KomaMRI, you can read the paper introducing it here). However, being developed fairly recently, there are still new features that can be added and optimization to be done.\n\n\nProject Goals\nThe goals outlined by Carlos (my project mentor) and I the beginning of this summer were:\n\nExtend GPU support beyond CUDA to include AMD, Intel, and Apple Silicon GPUs, through the packages AMDGPU.jl, oneAPI.jl, and Metal.jl\nCreate a CI pipeline to be able to test each of the GPU backends\nCreate a new kernel-based simulation method optimized for the GPU, which we expected would outperform array broadcasting\n(Stretch Goal) Look into ways to support running distributed simulations across multiple nodes or GPUs\n\n\n\nStep 1: Support for Different GPU backends\nPreviously, KomaMRI‚Äôs support for GPU acceleration worked by converting each array used within the simulation to a CuArray, the device array type defined in CUDA.jl. This was done through a general gpu function. The inner simulation code is GPU-agnostic, as the same operations can be performed on a CuArray or a plain CPU Array. This approach is good for extensibility, as it does not require writing different simulation code for the CPU / GPU, or different GPU backends, and would only work in a language like Julia based on runtime dispatch!\nTo extend this to multiple GPU backends, all that is needed is to generalize the gpu function to convert to either the device types of CUDA.jl, AMDGPU.jl, Metal.jl, or oneAPI.jl, depending on which backend is being used. To give an idea of what the gpu conversion code looked like before, here is a snippet:\nstruct KomaCUDAAdaptor end\nadapt_storage(to::KomaCUDAAdaptor, x) = CUDA.cu(x)\n\nfunction gpu(x)\n    check_use_cuda()\n    return use_cuda[] ? fmap(x -&gt; adapt(KomaCUDAAdaptor(), x), x; exclude=_isleaf) : x\nend\n\n#CPU adaptor\nstruct KomaCPUAdaptor end\nadapt_storage(to::KomaCPUAdaptor, x::AbstractArray) = adapt(Array, x)\nadapt_storage(to::KomaCPUAdaptor, x::AbstractRange) = x\n\ncpu(x) = fmap(x -&gt; adapt(KomaCPUAdaptor(), x), x)\nThe fmap function is from the package Functors.jl and can recursively apply a function to a struct tagged with @functor. The function being applied is adapt from Adapt.jl, which will call the lower-level adapt_storage function to actually convert to / from the device type. The second parameter to adapt is what is being adapted, and the first is what it is being adapted to, which in this case is a custom adapter struct KomaCUDAAdapter.\nOne possible approach to generalize to different backends would be to define additional adapter structs for each backend and corresponding adapt_storage functions. This is what the popular machine learning library Flux.jl does. However, there is a simpler way!\nEach backend package (CUDA.jl, Metal.jl, etc.) already defines adapt_storage functions for converting different types to / from corresponding device type. Reusing these functions is preferable to defining our own since, not only does it save work, but it allows us to rely on the expertise of the developers who wrote those packages! If there is an issue with types being converted incorrectly that is fixed in one of those packages, then we would not need to update our code to get this fix since we are using the definitions they created.\nOur final gpu and cpu functions are very simple. The backend parameter is a type derived from the abstract Backend type of KernelAbstractions.jl, which is extended by each of the backend packages:\nimport KernelAbstractions as KA\n\nfunction gpu(x, backend::KA.GPU)\n    return fmap(x -&gt; adapt(backend, x), x; exclude=_isleaf)\nend\n\ncpu(x) = fmap(x -&gt; adapt(KA.CPU(), x), x, exclude=_isleaf)\nThe other work needed to generalize our GPU support involved switching to use package extensions to avoid having each of the backend packages as an explicit dependency, and defining some basic GPU functions for backend selection and printing information about available GPU devices. The pull request for adding support for multiple backends is linked below:\n\nhttps://github.com/JuliaHealth/KomaMRI.jl/pull/405\n\n\n\nStep 2: Buildkite CI\nAt the time the above pull request was merged, we weren‚Äôt sure whether the added support for AMD and Intel GPUs actually worked, since we only had access to CUDA and Apple Silicon GPUs. So the next step was to set up a CI to test each GPU backend. To do this, we used Buildkite, which is a CI platform that many other Julia packages also use. Since there were many examples to follow, setting up our testing pipeline was not too difficult. Each step of the pipeline does the required environment setup and then calls Pkg.test() for KomaMRICore. As an example, here is what the AMDGPU step of our pipeline looks like:\n      - label: \"AMDGPU: Run tests on v{{matrix.version}}\"\n        matrix:\n          setup:\n            version:\n              - \"1\"\n        plugins:\n          - JuliaCI/julia#v1:\n              version: \"{{matrix.version}}\"\n          - JuliaCI/julia-coverage#v1:\n              codecov: true\n              dirs:\n                - KomaMRICore/src\n                - KomaMRICore/ext\n        command: |\n          julia -e 'println(\"--- :julia: Instantiating project\")\n              using Pkg\n              Pkg.develop([\n                  PackageSpec(path=pwd(), subdir=\"KomaMRIBase\"),\n                  PackageSpec(path=pwd(), subdir=\"KomaMRICore\"),\n              ])'\n          \n          julia --project=KomaMRICore/test -e 'println(\"--- :julia: Add AMDGPU to test environment\")\n              using Pkg\n              Pkg.add(\"AMDGPU\")'\n          \n          julia -e 'println(\"--- :julia: Running tests\")\n              using Pkg\n              Pkg.test(\"KomaMRICore\"; coverage=true, test_args=[\"AMDGPU\"])'\n        agents:\n          queue: \"juliagpu\"\n          rocm: \"*\"\n        timeout_in_minutes: 60\nWe also decided that in addition to a testing CI, it would also be helpful to have a benchmarking CI to track performance changes resulting from each commit to the main branch of the repository. Lux.jl had a very nice-looking benchmarking page, so I decided to look into their approach. They were using github-action-benchmark, a popular benchmarking action that integrates with the Julia package BenchmarkTools.jl. github-action-benchmark does two very useful things:\n\nCollects benchmarking data into a json file and provides a default index.html to display this data. If put inside a relative path in the gh-pages branch of a repository, this results in a public benchmarking page which is automatically updated after each commit!\nComments on a pull request with the benchmarking results compared with before the pull request. Example: https://github.com/JuliaHealth/KomaMRI.jl/pull/442#pullrequestreview-2213921334\n\nThe only issue was that since github-action-benchmark is a github action, it is meant to be run within github by one of the available github runners. While this works for CPU benchmarking, only Buildkite has the CI setup for each of the GPU backends we are using, and Lux.jl‚Äôs benchmarks page only included CPU benchmarks, not GPU benchmarks (Note: we talked with Avik, the repository owner of Lux.jl, and Lux.jl has since adopted the approach outlined below to display GPU and CPU benchmarks together). I was not able to find any examples of other julia packages using github-action-benchmark for GPU benchmarking.\nFortunately, there is a tool someone developed to download results from Buildkite into a github action (https://github.com/EnricoMi/download-buildkite-artifact-action). This repository only had 1 star when I found it, but it does exactly what we needed: it identifies the corresponding Buildkite build for a commit, waits for it to finish, and then downloads the artifacts for the build into the github action it is being run from. With this, we were able to download the Buildkite benchmark results from a final aggregation step into our benchmarking action and upload to github-action-benchmark to publish to either the main data.js file for our benchmarking website, or pull request.\nOur final benchmarking page looks like this and is publicly accessible:\n\nOne neat thing about github-action-benchmark is that the default index.html is extensible, so even though by deault it only shows time, the information for memory usage and number of allocations is also collected into the json file, and can be displayed as well.\nA successful CI run on Buildkite Looks like this:\n\nThe pull requests for creating the CI testing and benchmarking pipeline, and changing the index.html for our benchmark page are listed below:\n\nhttps://github.com/JuliaHealth/KomaMRI.jl/pull/411\nhttps://github.com/JuliaHealth/KomaMRI.jl/pull/418\nhttps://github.com/JuliaHealth/KomaMRI.jl/pull/421\n\n\n\nStep 3: Optimization\nWith support for multiple backends enabled, and a robust CI, the next step was to optimize our simulation code as much as possible. Our original idea was to create a new GPU-optimized simulation method, but before doing this we wanted to look more at the existing code and optimize for the CPU.\nThe simulation code is solving a differential equation (the [Bloch equations(https://en.wikipedia.org/wiki/Bloch_equations)]) over time. Most differential equation solvers step through time, updating the current state at each time step, but our previous simulation code, more optimized for the GPU, did a lot of computations across all time points in a simulation block, allocating a matrix of size Nspins by NŒît each time this was done. Although this is beneficial for the GPU, where there are millions of threads available on which to parallelize these computations, for the CPU it is more important to conserve memory, and the aforementioned approach of stepping through time is preferable.\nAfter seeing that this approach did help speed up simulation time on the CPU, but was not faster on the GPU (7x slower for Metal!) we decided to separate our simulation code for the GPU and CPU, dispatching based on the KernelAbstractions.Backend type depending on if it is &lt;:KernelAbstractions.CPU or &lt;:KernelAbstractions.GPU.\nOther things we were able to do to speed up CPU computation time:\n\nPreallocating each array used inside the core simulation code so it can be re-used from one simulation block to the next.\nSkipping an expensive computation if the magnetization at that time point is not added to the final signal\nEnsuring that each statement is fully broadcasted. We were surprised to see the difference between the following examples:\n\n#Fast\nBz = x .* seq.Gx' .+ y .* seq.Gy' .+ z .* seq.Gz' .+ p.Œîw ./ T(2œÄ .* Œ≥)\n\n#Slow\nBz = x .* seq.Gx' .+ y .* seq.Gy' .+ z .* seq.Gz' .+ p.Œîw / T(2œÄ * Œ≥)\n\nUsing the cis function for complex exponentiation, which is faster than exp\n\nWith these changes, the mean improvement in simulation time aggregating across each of our benchmarks for 1, 2, 4, and 8 CPU threads was ~4.28. For 1 thread, the average improvement in memory usage was 90x!\nThe next task was optimizing the simulation code for the GPU. Although our original idea was to put everything into one GPU kernel, we found that the existing broadcasting operations were already very fast, and that custom kernels we wrote were not able to outperform the previous implementation. The Julia GPU compiler team deserves a lot of credit for developing such fast broadcasting implementations!\nHowever, this does not mean that we were unable to improve the GPU simulation time. Similar to with the CPU, preallocation made a substantial difference. Parallelizing as much work as possible across the time points for a simulation block was also found to beneficial. For the parts that needed to be done sequentially, a custom GPU kernel was written which used the KernelAbstractions.@localmem macro for arrays being updated at each time step to yield faster memory access.\nThe mean speedup we saw across the 4 supported GPU backends was 4.16, although this varied accross each backend (for example, CUDA was only 2.66x faster while oneAPI was 28x faster). There is a remaining bottleneck in the run_spin_preceession! function having to do with logical indexing that I was not able to resolve, but could be solved in the future to speed up the GPU simulation time even further!\nThe pull requests optimizing code for the CPU and GPU are below:\n\nhttps://github.com/JuliaHealth/KomaMRI.jl/pull/443\nhttps://github.com/JuliaHealth/KomaMRI.jl/pull/459\nhttps://github.com/JuliaHealth/KomaMRI.jl/pull/462\n\n\n\n4. Step 4: Distributed Support\nThis last step was a stretch goal for exploring how to add distributed support to KomaMRI. MRI simulations can become quite large, so it is useful to be able to distribute work across either multiple GPUs or multiple compute nodes.\nA nice thing about MRI simulation is the independent spin property: if a phantom object (representing, for example a brain tissue slice) is divided into two parts, and each part is simulated separately, the signal result from simulating the whole phantom will be equal to the sum of the signal results from simulating each subdivision of the original phantom. This makes it quite easy to distribute work, either across more than one GPU or accross multiple compute nodes.\nThe following scripts worked, with the only necessary code change to the repository being a new + function to add two RawAcquisitionData structs:\n#Use multiple GPUs:\nusing Distributed\nusing CUDA\n\n#Add workers based on the number of available devices\naddprocs(length(devices()))\n\n#Define inputs on each worker process\n@everywhere begin\n    using KomaMRI, CUDA\n    sys = Scanner()\n    seq = PulseDesigner.EPI_example()\n    obj = brain_phantom2D()\n    #Divide phantom\n    parts = kfoldperm(length(obj), nworkers())\nend\n\n#Distribute simulation across workers\nraw = Distributed.@distributed (+) for i=1:nworkers()\n    KomaMRICore.set_device!(i-1) #Sets device for this worker, note that CUDA devices are indexed from 0\n    simulate(obj[parts[i]], seq, sys)\nend\n#Use multiple compute nodes\nusing Distributed\nusing ClusterManagers\n\n#Add workers based on the specified number of SLURM tasks\naddprocs(SlurmManager(parse(Int, ENV[\"SLURM_NTASKS\"])))\n\n#Define inputs on each worker process\n@everywhere begin\n    using KomaMRI\n    sys = Scanner()\n    seq = PulseDesigner.EPI_example()\n    obj = brain_phantom2D()\n    parts = kfoldperm(length(obj), nworkers())\nend\n\n#Distribute simulation across workers\nraw = Distributed.@distributed (+) for i=1:nworkers()\n    simulate(obj[parts[i]], seq, sys)\nend\nPull reqeust for adding these examples to the KomaMRI documentation: https://github.com/JuliaHealth/KomaMRI.jl/pull/468\n\n\nConclusions / Future Work\nThis project was a 350-hour large project, since there were many goals to accomplish. To summarize what changed since the beginning of the project:\n\nAdded support for AMDGPU.jl, Metal.jl, and oneAPI.jl GPU backends\nCI for automated testing and benchmarking accross each backend + public benchmarks page\nSignificantly faster CPU and GPU performance\nDemonstrated distributed support and examples added in documentation\n\nFuture work could look at ways to further optimize the simulation code, since despite the progress made, I believe there is more work to be done! The aforementioned logical indexing issue is still not resolved, and the kernel used inside the run_spin_excitation! function has not been profiled in depth. KomaMRI is also looking into adding support for higher-order ODE methods, which could require more GPU kernels being written.\n\n\nAcknowledgements\nI would like to thank my mentor, Carlos Castillo, for his help and support on this project. I would also like to thank Jakub Mitura, who attended some of our meetings to help with GPU optimization, Dilum Aluthge who helped set up our BuildKite pipeline, and Tim Besard, who answered many GPU-related questions that Carlos and I had.\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{kierulf2024,\n  author = {Kierulf, Ryan},\n  title = {GSoC ‚Äô24: {Enhancements} to {KomaMRI.jl} {GPU} {Support}},\n  date = {2024-08-30},\n  url = {https://juliahealth.org/JuliaHealthBlog/posts/ryan-gsoc/Ryan_GSOC.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nKierulf, Ryan. 2024. ‚ÄúGSoC ‚Äô24: Enhancements to KomaMRI.jl GPU\nSupport.‚Äù August 30, 2024. https://juliahealth.org/JuliaHealthBlog/posts/ryan-gsoc/Ryan_GSOC.html."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\nWelcome to the JuliaHealthBlog! üëã\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nGSoC ‚Äô24: Developing Tooling for Observational Health Research in Julia\n\n\n\n\n\nA summary of my project for Google Summer of Code - 2024\n\n\n\n\n\nSep 7, 2024\n\n\nJay Sanjay Landge\n\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\n\nGSoC ‚Äô24: Enhancements to KomaMRI.jl GPU Support\n\n\n\n\n\nA summary of my project for Google Summer of Code\n\n\n\n\n\nAug 30, 2024\n\n\nRyan Kierulf\n\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\n\nGSoC ‚Äô24: IPUMS.jl Small Project\n\n\n\n\n\nA summary of my project for Google Summer of Code\n\n\n\n\n\nAug 26, 2024\n\n\nMichela Rocchetti\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nDummy Post\n\n\n\n\n\nPost description\n\n\n\n\n\nJun 22, 2024\n\n\nFoobar\n\n\n1 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  }
]