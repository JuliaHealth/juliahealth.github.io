{
  "hash": "66fbb04c9988226e2d7bb3425dc91e35",
  "result": {
    "engine": "julia",
    "markdown": "---\ntitle: \"GSoC '24: Adding dataset-wide functions and integrations of augmentations\"\ndescription: \"MedPipe3D - Medical segmentation pipeline with dataset-wide functions and augmentations.\"\nauthor: \"Jan Zubik\"\ndate: \"11/03/2024\"\ntoc: true\nengine: julia\ncategories:\n  - gsoc\n  - AI/ML\n  - imaging\n  - gpu\n  - analysis\n---\n\n\n\n\n# ğŸ“ğŸ©»ğŸ“ğŸ“‰ â¡ï¸ ğŸ—ƒï¸ğŸ“šâ™»ï¸ğŸ§‘â€ğŸ« â¡ï¸ ğŸ¤–ğŸ‘ï¸ğŸ“ˆ â¡ï¸ â¤ï¸â€ğŸ©¹  \n*These emoticons may resemble **hieroglyphics**, but very soon you will realize that they **mean more than 1000s** of lines of code.*\n\n<details>\n  <summary>Description of the emojis used in the title</summary>\n  <ul>\n    <li>ğŸ“ **Action Plan**: A clear, structured plan that guides each step of the MedPipe3D pipeline.</li>\n    <li>ğŸ©» **3D Medical Images**: Medical imaging data, such as MRI scans in Nifti format.</li>\n    <li>ğŸ“ **AI Model**: The initial AI model that will be trained and refined within the pipeline.</li>\n    <li>ğŸ“‰ **Loss Function**: A function that measures the modelâ€™s performance during training, guiding the optimization process.</li>    \n    <li>ğŸ—ƒï¸ **Data Loading**: Preparation and loading of data and metadata into HDF5 format.</li>\n    <li>ğŸ“š **Data Splitting**: Dividing data into training, validation, and test sets.</li>\n    <li>â™»ï¸ **Data Augmentation**: Increasing data variability through augmentation.</li>\n    <li>ğŸ§‘â€ğŸ« **AI Training**: Using Lux.jl framework to train the AI model.</li>\n    <li>ğŸ¤– **Model**: The trained AI model that can perform tasks like segmentation on medical images.</li>\n    <li>ğŸ‘ï¸ **Data for Visualization**: Output data, such as masks and segmentations.</li>\n    <li>ğŸ“ˆ **Performance Logs**: Logs and metrics documenting the AI's performance.</li>\n    <li>â¤ï¸â€ğŸ©¹ **Purpose of MedPipe3D**</li>\n  </ul>\n\n</details>\n\n<hr>\nIn this post, I'd like to summarize what I did this summer and everything I learned along the way, rebuilding the MedPipe3D medical imaging pipeline. I will not start typically, but so that anyone even a novice can visualize what this project has achieved, while the latter part is intended for more experienced readers. It will be easiest to divide it into 4 steps separated by â¡ï¸ in the title above. Each emoji stands for a different piece of pipeliner and will be described below.\n\nğŸ“ğŸ©»ğŸ“ğŸ“‰ **What we need from the user**\n\nMedPipe3D requires four essential inputs from the user to get started: a clear action plan ğŸ“, 3D medical images like MRI scans ğŸ©», an AI model ğŸ“, and a loss function ğŸ“‰.\n\nğŸ—ƒï¸ğŸ“šâ™»ï¸ğŸ§‘â€ğŸ« **The Pipeline essential AI manufacturing line**\n\nFollowing the plan ğŸ“, MedPipe3D loads data, pre-processes, and organizes it ğŸ—ƒï¸. Allowing data to be easily split ğŸ“š, and efficiently augmented â™»ï¸ in many ways for learning AI ğŸ§‘â€ğŸ« model effectively. In the end, performing testing and post-processing for better determination of AI skills.  \nIt's designed to transform raw medical data into a format that your AI can learn from, segmenting meaningful patterns and structures.\n\nğŸ¤–ğŸ‘ï¸ğŸ“ˆ **Results and Insights**\n\nMedPipe3D is a tool for researchers and for that, it cannot do without analysis, testing, and evaluation. The result of the pipeline is a model ğŸ¤– as well as data ğŸ‘ï¸ and logs ğŸ“ˆ needed in MedEval3D that are ready for visualization and further analysis with MedEye3D. In a nutshell, it makes visualizing results easy, tumor locations or other medical features directly as masks on the scans.\n\nâ¤ï¸â€ğŸ©¹ **Purpose-Driven Technology**\n\nMedPipe3D's mission goes beyond technology. It's about providing the tools to create AIs that support healthcare professionals in making faster, more accurate decisions, with the ultimate goal of saving lives.\n\nThis four-part journey captures the heart of the MedPipe3D toolkit for advancing medical AI, from raw data to life-saving insight.\n\n## Introduction\n\n**MedPipe3D** is a framework created from hundreds of hours over summer vacation, thousands of lines of code, hundreds of mistakes, and most importantly the guidance of my mentor and author of all of these libraries Dr. [Jakub Mitura](https://www.linkedin.com/in/jakub-mitura-7b2013151/).\nAt its core, MedPipe3D combines sophisticated data handling from **MedImage** thanks to the hard work of [Divyansh Goyal](https://www.linkedin.com/in/divyansh-goyal-34654b200/). Newly developed pipeline for model training, validation, and testing with existing **MedEval3D**, and result visualization with **MedEye3D**.\nUnfortunately, not all of the project's goals have been fully achieved, and thereby there is one section â¡ï¸ too many. Hopefully not for long. My name is [Jan Zubik](https://www.linkedin.com/in/janzubik/), and I wrote this entire library from scratch, which is currently my most complex project.\n\nIf you are a data scientist, programmer, or code enthusiast, I invite you to read the next section where I go into detail and present **version 1** of this tool in detail.\n\nI'm a 3rd-year student of BSc in Data Science and Machine Learning, I know that many things can be done better, expanded, debugged, and optimized. Now it just works, **but don't hesitate to write to me personally** on [LinkedIn](https://www.linkedin.com/in/janzubik/), [Julia's Slack](https://julialang.slack.com/team/U06L685B6TD) or [GitHub](https://github.com/JanZubik)!\nWith your comments, and direct critique **you will help me** to be a better programmer and one day MedPipe3D will contribute in a tiny way to save someone's life!\n\nExact work from the Google Summer of Code project you will find in [GitHub the repository.](https://github.com/JuliaHealth/MedPipe3D.jl/tree/GSoC-'24-MedPipe3D)\n\n\n# Project Goals\n\nThe primary goal was to develop MedPipe3D and enhance MedImage, a Julia package designed to streamline the process of GPU-accelerated medical image segmentation. The project aimed to merge existing librariesâ€”MedEye3D, MedEval3D, and MedImageâ€”into a cohesive pipeline that facilitates advanced data handling, preprocessing, augmentation, model training, validation, testing with post-processing and visualization for medical imaging applications.\n\n\n\n# Tasks\n\n- ğŸ†™ - Fully finished, with great potential for further development\n- âœ… - Fully completed\n- âš ï¸ - Partially uncompleted\n- âŒ - Unreached\n\nFull list of all major parts and minor tasks (all tasks set up in the original GSOC plan were completed at least minimum level, and many additional improvements above minimum were implemented)\n<details>\n\n1. **Helpful functions to support the MedImage format âœ…**\n  - Debugging rotations âœ…\n  - Crop MedImage or 3D array âœ…\n  - Pad MedImage or 3D array âœ…\n  - Pad with edge values âœ…\n  - Calculating the average of the edges of the picture ğŸ†™\n\n2. **Integrate Augmentations for Medical Data âœ…**\n  - Brightness transform âœ…\n  - Contrast augmentation transform âœ…\n  - Gamma Transform âœ…\n  - Gaussian noise transform âœ…\n  - Rician noise transform âœ…\n  - Mirror transform âœ…\n  - Scale transform ğŸ†™\n  - Gaussian blur transform âœ…\n  - Simulate low-resolution transform ğŸ†™\n  - Elastic deformation transform ğŸ†™\n\n3. **Develop a Pipeline âš ï¸**\n  - Structured configuration of all hyperparameters ğŸ†™\n   - Interactive creation of configuration âœ…\n   - Creating a structured configuration of hyperparameters in JSON ğŸ†™\n  - Loading data into HDF5 âœ…\n    - Cropping and padding to real coordinates of the main picture âœ…\n    - Calculate Median and Mean Spacing with resampling ğŸ†™\n    - Cropping and padding to specific or average dimensions âœ…\n    - Standardization and normalization âœ…\n  - Managing index groups (channels) for batch requirements in HDF5 âœ…\n    - Divide into train, validation, test specified as % âœ…\n    - Divide with a specific division specified in JSON âœ…\n    - Equal distribution when there are multiple classes âœ…\n  - Extracting data and creating 5-dimensional tensors for batched learning âœ…\n    - Hole images data loading âœ…\n    - Patch-based data loading with probabilistic oversampling âœ…\n  - Obtaining the necessary elements for learning âœ…\n    - Get optimizer, loss function, and performance metrics âœ…\n  - Apply augmentations âœ…\n  - Train âœ…\n    - Initializing model âœ…\n    - The learning epoch âœ…\n    - Epoch with early stopping functionality âœ…\n  - Inferring âœ…\n  - Validation âœ…\n    - Evaluate metric âœ…\n    - Evaluate validation loss âœ…\n    - Validation with largest connected componentâœ…\n  - Testing âœ…\n    - Evaluate test set âœ…\n    - Invertible augmentations evaluation âœ…\n    - Patch-based invertible augmentations evaluation âœ…\n  - Logging âš ï¸\n    - Returning the necessary results âš ï¸\n    - Logging connection to TensorBoard âŒ\n    - Logging errors and warnings âŒ\n  - Visualization âš ï¸\n    - Returning data in Nifti format âœ…\n    - Automated visualization in MedEye3D âŒ\n\n4. **Optimize Performance with GPU Acceleration**\n   - Augmentations âœ…\n   - Learning, Validation, Testing âœ…\n   - Largest connected component âœ…\n\n5. **Documentation âš ï¸**\n   - Comments in important places in the code âš ï¸\n   - Documentation of the function âš ï¸\n   - Read me âš ï¸\n   - Documentation on juliahealth.org âŒ\n\n</details>\n\n## Integrate augmentations for medical data ğŸ†™\nAugmenting medical data is a crucial step for enhancing model robustness, especially given the variations in imaging conditions and patient anatomy. \n\n- This pipeline currently supports multiple augmentation techniques:\n  - Brightness transform âœ…\n  - Contrast augmentation transform âœ…\n  - Gamma Transform âœ…\n  - Gaussian noise transform âœ…\n  - Rician noise transform âœ…\n  - Mirror transform âœ…\n  - Scale transform ğŸ†™\n  - Gaussian blur transform âœ…\n  - Simulate low-resolution transform ğŸ†™\n  - Elastic deformation transform ğŸ†™\n\nWhich have been fully integrated. Each of these methods helps the model generalize better by simulating diverse imaging scenarios.\n\n![](./Augmentations.png)\n\nComments:\n\nAugmentations such as scaling, and low-resolution simulation use interpolation that is not yet GPU-accelerated.\n\nElastic deformation with simulation of different tissue elasticities is a potential development opportunity that would further improve the model's adaptability by mimicking more complex variations found in medical imaging.\n\n## Invertible augmentations and support test time augmentations ğŸ†™\nThis section focuses on the ability to apply reversible augmentations to test data, allowing the model to be evaluated with different transformations. Only rotation is available at this time. The function `evaluate_patches` performs this evaluation by applying specified augmentations, dividing the test data into patches, and reconstructing the full image from the patches. During testing, one can choose to use of largest connected component post-processing. Metrics are calculated and results are saved for analysis.\n\n<details>\n<summary>evaluate_test:</summary>\n\n```julia\n# ...\nfor test_group in test_groups\n    test_data, test_label, attributes = fetch_and_preprocess_data([test_group], h5, config)\n    results, test_metrics = evaluate_patches(test_data, test_label,  tstate, model, config)\n    y_pred, metr = process_results(results, test_metrics, config)\n    save_results(y_pred, attributes, config)\n    push!(all_test_metrics, metr)\nend\n# ...\n```\n\n```julia\nfunction evaluate_patches(test_data, test_label, tstate, model, config, axis, angle)\n    println(\"Evaluating patches...\")\n    results = []\n    test_metrics = []\n    tstates = [tstate]\n    test_time_augs = []\n\n    for i in config[\"learning\"][\"n_invertible\"]\n        data = rotate_mi(test_data, axis, angle)\n        for tstate_curr in tstates\n            patch_results = []\n            patch_size = Tuple(config[\"learning\"][\"patch_size\"])\n            idx_and_patches, paded_data_size = divide_into_patches(test_data, patch_size)\n            coordinates = [patch[1] for patch in idx_and_patches]\n            patch_data = [patch[2] for patch in idx_and_patches]\n            for patch in patch_data\n                y_pred_patch, _ = infer_model(tstate_curr, model, patch)\n                push!(patch_results, y_pred_patch)\n            end\n            idx_and_y_pred_patch = zip(coordinates, patch_results)\n            y_pred = recreate_image_from_patches(idx_and_y_pred_patch, paded_data_size, patch_size, size(test_data))\n            if config[\"learning\"][\"largest_connected_component\"]\n                y_pred = largest_connected_component(y_pred, config[\"learning\"][\"n_lcc\"])\n            end\n            metr = evaluate_metric(y_pred, test_label, config[\"learning\"][\"metric\"])\n            push!(test_metrics, metr)\n        end\n    end\n    return results, test_metrics\nend\n```\n\n```julia\nfunction divide_into_patches(image::AbstractArray{T, 5}, patch_size::Tuple{Int, Int, Int}) where T\n    println(\"Dividing image into patches...\")\n    println(\"Size of the image: \", size(image)) \n\n    # Calculate the required padding for each dimension (W, H, D)\n    pad_size = (\n        (size(image, 1) % patch_size[1]) != 0 ? patch_size[1] - size(image, 1) % patch_size[1] : 0,\n        (size(image, 2) % patch_size[2]) != 0 ? patch_size[2] - size(image, 2) % patch_size[2] : 0,\n        (size(image, 3) % patch_size[3]) != 0 ? patch_size[3] - size(image, 3) % patch_size[3] : 0\n    )\n\n    # Pad the image if necessary\n    padded_image = image\n    if any(pad_size .> 0)\n        padded_image = crop_or_pad(image, (size(image, 1) + pad_size[1], size(image, 2) + pad_size[2], size(image, 3) + pad_size[3]))\n    end\n\n    # Extract patches\n    patches = []\n    for x in 1:patch_size[1]:size(padded_image, 1)\n        for y in 1:patch_size[2]:size(padded_image, 2)\n            for z in 1:patch_size[3]:size(padded_image, 3)\n                patch = view(\n                    padded_image,\n                    x:min(x+patch_size[1]-1, size(padded_image, 1)),\n                    y:min(y+patch_size[2]-1, size(padded_image, 2)),\n                    z:min(z+patch_size[3]-1, size(padded_image, 3)),\n                    :,\n                    :\n                )\n                push!(patches, [(x, y, z), patch])\n            end\n        end\n    end\n    println(\"Size of padded image: \", size(padded_image))\n    return patches, size(padded_image)\nend\n\nfunction recreate_image_from_patches(\n    coords_with_patches,\n    padded_size,\n    patch_size,\n    original_size\n)\n    println(\"Recreating image from patches...\")\n    reconstructed_image = zeros(Float32, padded_size...)\n    \n    # Place patches back into their original positions\n    for (coords, patch) in coords_with_patches\n        x, y, z = coords\n        reconstructed_image[\n            x:x+patch_size[1]-1,\n            y:y+patch_size[2]-1,\n            z:z+patch_size[3]-1,\n            :,\n            :\n        ] = patch\n    end\n\n    # Crop the reconstructed image to remove any padding\n    final_image = reconstructed_image[\n        1:original_size[1],\n        1:original_size[2],\n        1:original_size[3],\n        :,\n        :\n    ]\n    println(\"Size of the final image: \", size(final_image))\n    return final_image\nend\n```\n</details>\n\nComment:<br>\nIn this section, there is significant potential to incorporate additional types of invertible augmentations.\n\n## Patch-based data loading with probabilistic oversampling âœ…\nIn this section, patches are extracted using `extract_patch` from the medical images for model training, with a probability-based method to decide between a random patch or a patch with non-zero labels.\nHelper functions like `get_random_patch` and `get_centered_patch` determine the starting indices and dimensions for the patches based on given configurations, while padding methods ensure consistency even if the patch exceeds the original image dimensions. Probabilistic oversampling, as configured, allows for more balanced and informative data sampling, which improves the model's ability to detect specific medical features.\n\n\n<details>\n<summary>extract_patch:</summary>\n\n```julia\nfunction extract_patch(image, label, patch_size, config)\n    # Fetch the oversampling probability from the config\n    println(\"Extracting patch.\")\n    oversampling_probability = config[\"learning\"][\"oversampling_probability\"]\n    # Generate a random number to decide which patch extraction method to use\n    random_choice = rand()\n\n    if random_choice <= oversampling_probability\n        return extract_nonzero_patch(image, label, patch_size)\n    else\n\n        return get_random_patch(image, label, patch_size)\n    end\nend\n#Helper function, in case the mask is emptyClick to apply\nfunction extract_nonzero_patch(image, label, patch_size)\n    println(\"Extracting a patch centered around a non-zero label value.\")\n    indices = findall(x -> x != 0, label)\n    if isempty(indices)\n        # Fallback to random patch if no non-zero points are found\n        return get_random_patch(image, label, patch_size)\n    else\n        # Choose a random non-zero index to center the patch around\n        center = indices[rand(1:length(indices))]\n        return get_centered_patch(image, label, center, patch_size)\n    end\nend\n# Function to get a patch centered around a specific index\nfunction get_centered_patch(image, label, center, patch_size)\n    center_coords = Tuple(center)\n    half_patch = patch_size .Ã· 2\n    start_indices = center_coords .- half_patch\n    end_indices = start_indices .+ patch_size .- 1\n\n    # Calculate padding needed\n    pad_beg = (\n        max(1 - start_indices[1], 0),\n        max(1 - start_indices[2], 0),\n        max(1 - start_indices[3], 0)\n    )\n    pad_end = (\n        max(end_indices[1] - size(image, 1), 0),\n        max(end_indices[2] - size(image, 2), 0),\n        max(end_indices[3] - size(image, 3), 0)\n    )\n\n    # Adjust start_indices and end_indices after padding\n    start_indices_adj = start_indices .+ pad_beg\n    end_indices_adj = end_indices .+ pad_beg\n\n    # Convert padding values to integers\n    pad_beg = Tuple(round.(Int, pad_beg))\n    pad_end = Tuple(round.(Int, pad_end))\n\n    # Pad the image and label using pad_mi\n    image_padded = pad_mi(image, pad_beg, pad_end, 0)\n    label_padded = pad_mi(label, pad_beg, pad_end, 0)\n\n    # Extract the patch\n    image_patch = image_padded[\n        start_indices_adj[1]:end_indices_adj[1],\n        start_indices_adj[2]:end_indices_adj[2],\n        start_indices_adj[3]:end_indices_adj[3]\n    ]\n    label_patch = label_padded[\n        start_indices_adj[1]:end_indices_adj[1],\n        start_indices_adj[2]:end_indices_adj[2],\n        start_indices_adj[3]:end_indices_adj[3]\n    ]\n\n    return image_patch, label_patch\nend\n\nfunction get_random_patch(image, label, patch_size)\n    println(\"Extracting a random patch.\")\n    # Check if the patch size is greater than the image dimensions\n    if any(patch_size .> size(image))\n        # Calculate the needed size to fit the patch\n        needed_size = map(max, size(image), patch_size)\n        # Use crop_or_pad to ensure the image and label are at least as large as needed_size\n        image = crop_or_pad(image, needed_size)\n        label = crop_or_pad(label, needed_size)\n    end\n\n    # Calculate random start indices within the new allowable range\n    start_x = rand(1:size(image, 1) - patch_size[1] + 1)\n    start_y = rand(1:size(image, 2) - patch_size[2] + 1)\n    start_z = rand(1:size(image, 3) - patch_size[3] + 1)\n    start_indices = [start_x, start_y, start_z]\n    end_indices = start_indices .+ patch_size .- 1\n\n    # Extract the patch directly when within bounds\n    image_patch = image[start_indices[1]:end_indices[1], start_indices[2]:end_indices[2], start_indices[3]:end_indices[3]]\n    label_patch = label[start_indices[1]:end_indices[1], start_indices[2]:end_indices[2], start_indices[3]:end_indices[3]]\n\n    return image_patch, label_patch\nend\n\n```\n</details>\n\n## Calculate Median and Mean Spacing with resampling ğŸ†™\nThis part ensures that all images in the dataset have consistent real coordinates, spacing, and shape. It's a critical factor in medical imaging for accurate analysis. Calculating and applying set values, median or mean across images ensures uniformity.\n\n#### Resample images to target image ğŸ†™\nThis step aligns each image to the reference coordinates of the main image, ensuring that all images share a common spatial alignment. The `resample_to_image` function from MedImage.jl is used here, applying interpolation to adjust each image.\n\n\n<details>\n<summary>resample_images_to_target:</summary>\n\n```julia\nif resample_images_to_target && !isempty(Med_images)\n    println(\"Resampling $channel_type files in channel '$channel_folder' to the first $channel_type in the channel.\")\n    reference_image = Med_images[1]\n    Med_images = [resample_to_image(reference_image, img, interpolator) for img in Med_images]\nend\n```\n</details>\n\nComment:<br>\n`Resample_to_image` uses interpolation that is not yet GPU-accelerated in this implementation, this step slows down the data preparation phase significantly.\n\n#### Ensure uniform spacing across the entire dataset ğŸ†™\nThis step brings all images to a consistent voxel spacing across the dataset using `resample_to_spacing` from MedImage.jl. This uniform spacing is crucial for creating a standardized dataset where each image voxel represents the same physical volume.\n\n\n<details>\n<summary>esample_to_spacing:</summary>\n\n```julia\nif resample_images_spacing == \"set\"\n    println(\"Resampling all $channel_type files to target spacing: $target_spacing\")\n    target_spacing = Tuple(Float32(s) for s in target_spacing)\n    channels_data = [[resample_to_spacing(img, target_spacing, interpolator) for img in channel] for channel in channels_data]\nelseif resample_images_spacing == \"avg\"\n    println(\"Calculating average spacing across all $channel_type files and resampling.\")\n    all_spacings = [img.spacing for channel in channels_data for img in channel]\n    avg_spacing = Tuple(Float32(mean(s)) for s in zip(all_spacings...))\n    println(\"Average spacing calculated: $avg_spacing\")\n    channels_data = [[resample_to_spacing(img, avg_spacing, interpolator) for img in channel] for channel in channels_data]\nelseif resample_images_spacing == \"median\"\n    println(\"Calculating median spacing across all $channel_type files and resampling.\")\n    all_spacings = [img.spacing for channel in channels_data for img in channel]\n    median_spacing = Tuple(Float32(median(s)) for s in all_spacings)\n    println(\"Median spacing calculated: $median_spacing\")\n    channels_data = [[resample_to_spacing(img, median_spacing, interpolator) for img in channel] for channel in channels_data]\nelseif resample_images_spacing == false\n    println(\"Skipping resampling of $channel_type files.\")\n    # No resampling will be applied, channels_data remains unchanged.\nend\n```\n</details>\n\nComment:<br>\n`Resample_to_spacing` uses interpolation that is not yet GPU-accelerated in this implementation, this step slows down the data preparation phase significantly.\n\n#### Resizing all channel files to average or target size âœ…\nTo create a cohesive 5D tensor, all images in each channel are resized to a uniform shape, either the average size of all images or a specific target size. This resizing process uses `crop_or_pad`, ensuring that all images match the specified dimensions, making them suitable for model input.\n\n<details>\n<summary>crop_or_pad:</summary>\n\n```julia\nif resample_size == \"avg\"\n    sizes = [size(img.voxel_data) for img in channels_data for img in img]  # Get sizes from all images\n    avg_dim = map(mean, zip(sizes...))\n    avg_dim = Tuple(Int(round(d)) for d in avg_dim)\n    println(\"Resizing all $channel_type files to average dimension: $avg_dim\")\n    channels_data = [[crop_or_pad(img, avg_dim) for img in channel] for channel in channels_data]\nelseif resample_size != \"avg\"\n    target_dim = Tuple(resample_size)\n    println(\"Resizing all $channel_type files to target dimension: $target_dim\")\n    channels_data = [[crop_or_pad(img, target_dim) for img in channel] for channel in channels_data]\nend\n```\n</details>\n\n## Basic Post-processing operations\nPost-processing operations involve the algorithm `largest_connected_components`. It is achieved by label initialization and propagation in the segmented mask.\nThe `initialize_labels_kernel` function assigns unique labels to different regions.\n\n<details>\n<summary>initialize_labels_kernel:</summary>\n\n```julia\n@kernel function initialize_labels_kernel(mask, labels, width, height, depth)\n    idx = @index(Global, Cartesian)\n    i = idx[1]\n    j = idx[2]\n    k = idx[3]\n    \n    if i >= 1 && i <= width && j >= 1 && j <= height && k >= 1 && k <= depth\n        if mask[i, j, k] == 1\n            labels[i, j, k] = i + (j - 1) * width + (k - 1) * width * height\n        else\n            labels[i, j, k] = 0\n        end\n    end\nend\n```\n</details>\nPropagate_labels_kernel iteratively updates the labels to maintain connected regions.\npropagate_labels_kernel:\n<details>\n\n```julia\n@kernel function propagate_labels_kernel(mask, labels, width, height, depth)\n    idx= @index(Global, Cartesian)\n    i = idx[1]\n    j = idx[2]\n    k = idx[3]\n\n    if i >= 1 && i <= width && j >= 1 && j <= height && k >= 1 && k <= depth\n        if mask[i, j, k] == 1\n            current_label = labels[i, j, k]\n            for di in -1:1\n                for dj in -1:1\n                    for dk in -1:1\n                        if di == 0 && dj == 0 && dk == 0\n                            continue\n                        end\n                        ni = i + di\n                        nj = j + dj\n                        nk = k + dk\n                        if ni >= 1 && ni <= width && nj >= 1 && nj <= height && nk >= 1 && nk <= depth\n                            if mask[ni, nj, nk] == 1 && labels[ni, nj, nk] < current_label\n                                labels[i, j, k] = labels[ni, nj, nk]\n                            end\n                        end\n                    end\n                end\n            end\n        end\n    end\nend\n```\n</details>\nThis process facilitates the identification of the largest connected components in 3D space, helping to isolate relevant medical structures, such as tumors, in the segmented mask. Allowing determining how many such areas are to be returned.\n\n<details>\n<summary>largest_connected_components:</summary>\n\n```julia\nfunction largest_connected_components(mask::Array{Int32, 3}, n_lcc::Int)\n    width, height, depth = size(mask)\n    mask_gpu = CuArray(mask)\n    labels_gpu = CUDA.fill(0, size(mask))\n    dev = get_backend(labels_gpu)\n    ndrange = (width, height, depth)\n    workgroupsize = (3, 3, 3)\n\n    # Initialize labels\n    initialize_labels_kernel(dev)(mask_gpu, labels_gpu, width, height, depth, ndrange = ndrange)\n    CUDA.synchronize()\n\n    # Propagate labels iteratively\n    for _ in 1:10 \n        propagate_labels_kernel(dev, workgroupsize)(mask_gpu, labels_gpu, width, height, depth, ndrange = ndrange)\n        CUDA.synchronize()\n    end\n\n    # Download labels back to CPU\n    labels_cpu = Array(labels_gpu)\n    \n    # Find all unique labels and their sizes\n    unique_labels = unique(labels_cpu)\n    label_sizes = [(label, count(labels_cpu .== label)) for label in unique_labels if label != 0]\n\n    # Sort labels by size and get the top n_lcc\n    sort!(label_sizes, by = x -> x[2], rev = true)\n    top_labels = label_sizes[1:min(n_lcc, length(label_sizes))]\n\n    # Create a mask for each of the top n_lcc components\n    components = [labels_cpu .== label[1] for label in top_labels]\n    return components\nend\n```\n</details>\n\n## Structured configuration of all hyperparameters ğŸ†™\n\nHyperparameters for the entire pipeline are stored in a JSON configuration file, enabling straightforward adjustments for experimentation (just swap values, save and resume the study). This structured setup allows easy modification of key parameters, such as data set preparation, training settings, data augmentation, and resampling options.\n\n\n<details>\n<summary>Example configuration:</summary>\n\n```JSON\n{\n    \"model\": {\n        \"patience\": 10,\n        \"early_stopping_metric\": \"val_loss\",\n        \"optimizer_name\": \"Adam\",\n        \"loss_function_name\": \"l1\",\n        \"early_stopping\": true,\n        \"early_stopping_min_delta\": 0.01,\n        \"optimizer_args\": \"lr=0.001\",\n        \"num_epochs\": 10\n    },\n    \"data\": {\n        \"batch_complete\": false,\n        \"resample_size\": [200,101,49],\n        \"resample_to_target\": false,\n        \"resample_to_spacing\": false,\n        \"batch_size\": 3,\n        \"standardization\": false,\n        \"target_spacing\": null,\n        \"channel_size\": 1,\n        \"normalization\": false,\n        \"has_mask\": true\n    },\n    \"augmentation\": {\n        \"augmentations\": {\n            \"Brightness transform\": {\n                \"mode\": \"additive\",\n                \"value\": 0.2\n            }\n        },\n        \"p_rand\": 0.5,\n        \"processing_unit\": \"GPU\",\n        \"order\": [\n            \"Brightness transform\"\n        ]\n    },\n    \"learning\": {\n        \"Train_Val_Test_JSON\": false,\n        \"largest_connected_component\": false,\n        \"n_lcc\": 1,\n        \"n_folds\": 3,\n        \"invertible_augmentations\": false,\n        \"n_invertible\": true,\n        \n        \"class_JSON_path\": false,\n        \"additional_JSON_path\": false,\n        \"patch_size\": [50,50,50],\n        \"metric\": \"dice\",\n        \"n_cross_val\": false,\n        \"patch_probabilistic_oversampling\": false,\n        \"oversampling_probability\": 1.0,\n        \"test_train_validation\": [\n            0.6,\n            0.2,\n            0.2\n        ],\n        \"shuffle\": false\n    }\n}\n\n```\n</details>\n\nComments:<br>\nThe current configuration is loaded as a dictionary, which simplifies access and modification. This setup presents a strong foundation for integrating automated search algorithms for hyperparameter tuning, enabling more efficient model optimization.<br>\nThe configuration structure could be reorganized and re-named to improve readability, making it easier for users to locate and adjust specific parameters.\n\n## Visualization of algorithm outputs âš ï¸\nThis module provides basic visualization functionality by saving output masks and images first to MedImage format and then to Nifti format. The `create_nii_from_medimage` function from MedImage.jl generates Nifti files, which can be loaded into MedEye3D for 3D visualization.\n\nComments:<br>\nIntegrating this visualization module more fully with the pipeline could eliminate unnecessary steps. By automatically loading output masks and images as raw data into MedEye3D for 3D visualization and supporting a more efficient end-to-end workflow. \n\n## K-fold cross-validation functionality âœ…\nK-fold cross-validation is implemented to evaluate model performance more robustly. The data is split into multiple folds, with each fold serving as a validation set once, while the others form the training set. This functionality provides a better assessment of model performance across different subsets of the data.\n\n<details>\n<summary>K-fold cross-validation functionality:</summary>\n\n```julia\n...\n  tstate = initialize_train_state(rng, model, optimizer)\n  if config[\"learning\"][\"n_cross_val\"]\n      n_folds = config[\"learning\"][\"n_folds\"]\n      all_tstate = []\n      combined_indices = [indices_dict[\"train\"]; indices_dict[\"validation\"]]\n      shuffled_indices = shuffle(rng, combined_indices)\n      for fold in 1:n_folds\n          println(\"Starting fold $fold/$n_folds\")\n          train_groups, validation_groups = k_fold_split(combined_indices, n_folds, fold, rng)\n          \n          tstate = initialize_train_state(rng, model, optimizer)\n          final_tstate = epoch_loop(num_epochs, train_groups, validation_groups, h5, model, tstate, config, loss_function, num_classes)\n          \n          push!(all_tstate, final_tstate)\n      end\n  else\n      final_tstate = epoch_loop(num_epochs, train_groups, validation_groups, h5, model, tstate, config, loss_function, num_classes)\n  end\n  return final_tstate\n...  \n```\n</details>\n\nThe `k_fold_split` function organizes the indices for each fold, ensuring comprehensive coverage of the dataset during training.\n\n<details>\n<summary>k_fold_split</summary>\n\n```julia\nfunction k_fold_split(data, n_folds, current_fold)\n    fold_size = length(data) Ã· n_folds\n    validation_start = (current_fold - 1) * fold_size + 1\n    validation_end = validation_start + fold_size - 1\n    validation_indices = data[validation_start:validation_end]\n    train_indices = [data[1:validation_start-1]; data[validation_end+1:end]]\n    return train_indices, validation_indices\nend\n```\n</details>\n\n# Conclusions and Future Development\nI have successfully established a foundation for a medical imaging pipeline, addressing significant challenges in data handling, model training, and augmentation integration. The integration of dataset-wide functions has significantly enhanced the reproducibility and handling of batched data with GPU support enabling scalability of experiments, making it easier for researchers and practitioners to produce better results.\n\n# Future Development\nAs we look to the future, there are several areas where MedPipe3D can be expanded and improved to better serve the medical AI community. These include:\n\n## Necessary Enhancements\n\nComprehensive Logging: Develop detailed logging mechanisms that capture a wide range of events, including system statuses, model performance metrics, and user activities, to facilitate debugging and system optimization. This is currently executed as a simple `println` function.\n\nTensorBoard Integration: Implement an interface for TensorBoard to allow users to visualize training dynamics in real time, providing insights into model behavior and performance trends.\n\nError and Warning Logs: Introduce advanced error and warning logging capabilities to alert users of potential issues before they affect the pipeline's performance, ensuring smoother operations and maintenance.\n\nAutomated Visualization: Integrate MedEye3D directly into MedPipe3D to enable automated visualization of outputs, such as segmentation masks or other relevant medical imaging features. This feature would provide users with real-time visual feedback on model performance and data quality.\nCode-Level Documentation: Due to needed changes in the fundamental structure of the pipeline in the final phase of the project, it is necessary to reevaluate all documentation.\n\nOfficial JuliaHealth Documentation: Extend the documentation efforts to include official entries on juliahealth.org, providing a centralized and authoritative resource for users seeking to learn more about MedPipe3D and its capabilities with examples shown\n\n## Potential Enhancements\nGPU support for interpolation will allow for significant acceleration of such functions as Scale transform, Simulate, Low-resolution transform, Elastic deformation transform, and Resampling spacing.\n\nAdd more reversible augmentations to test time.\n\nCalculating the average of the edges of the picture: checking the type of photo and calculating more correctly on this basis\n\nElastic deformation transforms with the simulation of different tissue elasticities.\n\n# Acknowledgments ğŸ™‡â€â™‚ï¸\n\nI would like to express my deepest gratitude to my mentor Dr. [Jakub Mitura](https://www.linkedin.com/in/jakub-mitura-7b2013151/) for his invaluable guidance and support throughout this project. His expertise and encouragement were instrumental in overcoming challenges and achieving project milestones.\n\n",
    "supporting": [
      "GSoC_Jan_Zubik_MedPipe3D_files"
    ],
    "filters": [],
    "includes": {}
  }
}